{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x[train_idx])\n",
    "    loss = F.nll_loss(out, y_true.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv', root='../dataset')\n",
    "split_idx = dataset.get_idx_split()\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = data.y.to(device)\n",
    "train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90941])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(x.size(-1), 256, dataset.num_classes, 3, 0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Expected input format of Evaluator for ogbn-arxiv\n",
      "{'y_true': y_true, 'y_pred': y_pred}\n",
      "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
      "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
      "where y_pred stores predicted class label (integer),\n",
      "num_task is 1, and each row corresponds to one node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.expected_input_format) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 50, Loss: 1.6400, Train: 55.36%, Valid: 55.23%, Test: 53.00%\n",
      "Run: 01, Epoch: 100, Loss: 1.5435, Train: 58.76%, Valid: 56.30%, Test: 54.02%\n",
      "Run: 01, Epoch: 150, Loss: 1.4916, Train: 60.67%, Valid: 56.97%, Test: 54.56%\n",
      "Run: 01, Epoch: 200, Loss: 1.4680, Train: 61.85%, Valid: 57.35%, Test: 55.06%\n",
      "Run: 01, Epoch: 250, Loss: 1.4457, Train: 62.67%, Valid: 57.55%, Test: 55.05%\n",
      "Run: 01, Epoch: 300, Loss: 1.4318, Train: 63.18%, Valid: 57.61%, Test: 55.30%\n",
      "Run: 01, Epoch: 350, Loss: 1.4197, Train: 63.61%, Valid: 57.79%, Test: 55.69%\n",
      "Run: 01, Epoch: 400, Loss: 1.4183, Train: 63.90%, Valid: 58.05%, Test: 55.77%\n",
      "Run: 01, Epoch: 450, Loss: 1.4122, Train: 64.07%, Valid: 57.85%, Test: 55.62%\n",
      "Run: 01, Epoch: 500, Loss: 1.4088, Train: 64.39%, Valid: 57.80%, Test: 55.81%\n",
      "Run: 02, Epoch: 50, Loss: 1.6403, Train: 55.02%, Valid: 54.78%, Test: 52.69%\n",
      "Run: 02, Epoch: 100, Loss: 1.5402, Train: 58.81%, Valid: 56.57%, Test: 54.34%\n",
      "Run: 02, Epoch: 150, Loss: 1.4869, Train: 60.75%, Valid: 57.03%, Test: 54.90%\n",
      "Run: 02, Epoch: 200, Loss: 1.4636, Train: 61.99%, Valid: 57.55%, Test: 55.58%\n",
      "Run: 02, Epoch: 250, Loss: 1.4428, Train: 62.71%, Valid: 57.35%, Test: 55.56%\n",
      "Run: 02, Epoch: 300, Loss: 1.4344, Train: 63.36%, Valid: 57.58%, Test: 55.20%\n",
      "Run: 02, Epoch: 350, Loss: 1.4232, Train: 63.55%, Valid: 57.42%, Test: 55.28%\n",
      "Run: 02, Epoch: 400, Loss: 1.4120, Train: 63.91%, Valid: 57.75%, Test: 55.67%\n",
      "Run: 02, Epoch: 450, Loss: 1.4091, Train: 64.31%, Valid: 57.61%, Test: 55.36%\n",
      "Run: 02, Epoch: 500, Loss: 1.4079, Train: 64.34%, Valid: 57.61%, Test: 55.26%\n",
      "Run: 03, Epoch: 50, Loss: 1.6439, Train: 55.14%, Valid: 54.61%, Test: 52.67%\n",
      "Run: 03, Epoch: 100, Loss: 1.5443, Train: 58.64%, Valid: 56.43%, Test: 54.65%\n",
      "Run: 03, Epoch: 150, Loss: 1.4930, Train: 60.67%, Valid: 57.00%, Test: 55.17%\n",
      "Run: 03, Epoch: 200, Loss: 1.4624, Train: 61.92%, Valid: 57.34%, Test: 55.77%\n",
      "Run: 03, Epoch: 250, Loss: 1.4498, Train: 62.81%, Valid: 57.49%, Test: 55.39%\n",
      "Run: 03, Epoch: 300, Loss: 1.4321, Train: 63.32%, Valid: 57.65%, Test: 55.68%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-f0a9f12fee38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8f5d5e856673>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, x, y_true, train_idx, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(1, 1 + 500):\n",
    "        loss = train(model, x, y_true, train_idx, optimizer)\n",
    "        result = test(model, x, y_true, split_idx, evaluator)\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            train_acc, valid_acc, test_acc = result\n",
    "            print(f'Run: {run + 1:02d}, '\n",
    "                  f'Epoch: {epoch:02d}, '\n",
    "                  f'Loss: {loss:.4f}, '\n",
    "                  f'Train: {100 * train_acc:.2f}%, '\n",
    "                  f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                  f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90941, 128])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[train_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
