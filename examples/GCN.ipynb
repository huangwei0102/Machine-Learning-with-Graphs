{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_sparse\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(Conv, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_channels, out_channels))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        out = torch_sparse.matmul(adj, x) + self.bias\n",
    "        return out\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels, hidden_channels, num_layers, p):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(Conv(input_channels, hidden_channels))\n",
    "        \n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(Conv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "            \n",
    "        self.convs.append(Conv(hidden_channels, out_channels))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, adj)\n",
    "            x = self.bns[i](x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        out = self.convs[-1](x, adj)\n",
    "        return out\n",
    "\n",
    "def train(model, data, train_idx, optimizer, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    outputs = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = criterion(outputs, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = model(data.x, data.adj_t)\n",
    "    y_pred = outputs.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "hidden_channels = 256\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "epochs = 500\n",
    "log_steps = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv', root='../dataset', transform=T.ToSparseTensor())\n",
    "\n",
    "data = dataset[0]\n",
    "data.adj_t = data.adj_t.to_symmetric()\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(data.num_features, dataset.num_classes, hidden_channels, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 10, Loss: 1.3851, Train: 35.37%, Valid: 34.71% Test: 39.87%\n",
      "Run: 01, Epoch: 20, Loss: 1.1905, Train: 52.70%, Valid: 53.25% Test: 56.23%\n",
      "Run: 01, Epoch: 30, Loss: 1.1039, Train: 67.09%, Valid: 67.37% Test: 66.99%\n",
      "Run: 01, Epoch: 40, Loss: 1.0493, Train: 69.70%, Valid: 69.67% Test: 69.31%\n",
      "Run: 01, Epoch: 50, Loss: 1.0107, Train: 70.95%, Valid: 70.57% Test: 69.89%\n",
      "Run: 01, Epoch: 60, Loss: 0.9852, Train: 71.93%, Valid: 70.49% Test: 68.60%\n",
      "Run: 01, Epoch: 70, Loss: 0.9617, Train: 72.53%, Valid: 71.43% Test: 70.59%\n",
      "Run: 01, Epoch: 80, Loss: 0.9429, Train: 72.91%, Valid: 70.90% Test: 69.23%\n",
      "Run: 01, Epoch: 90, Loss: 0.9266, Train: 73.33%, Valid: 71.32% Test: 69.73%\n",
      "Run: 01, Epoch: 100, Loss: 0.9112, Train: 73.71%, Valid: 72.08% Test: 71.25%\n",
      "Run: 01, Epoch: 110, Loss: 0.8974, Train: 74.20%, Valid: 72.21% Test: 70.75%\n",
      "Run: 01, Epoch: 120, Loss: 0.8858, Train: 74.60%, Valid: 72.03% Test: 71.04%\n",
      "Run: 01, Epoch: 130, Loss: 0.8737, Train: 74.90%, Valid: 71.77% Test: 70.00%\n",
      "Run: 01, Epoch: 140, Loss: 0.8640, Train: 74.93%, Valid: 72.02% Test: 71.11%\n",
      "Run: 01, Epoch: 150, Loss: 0.8527, Train: 75.27%, Valid: 72.17% Test: 70.72%\n",
      "Run: 01, Epoch: 160, Loss: 0.8408, Train: 75.17%, Valid: 71.38% Test: 69.30%\n",
      "Run: 01, Epoch: 170, Loss: 0.8326, Train: 75.48%, Valid: 69.91% Test: 67.07%\n",
      "Run: 01, Epoch: 180, Loss: 0.8275, Train: 76.07%, Valid: 72.78% Test: 72.14%\n",
      "Run: 01, Epoch: 190, Loss: 0.8194, Train: 76.11%, Valid: 72.22% Test: 70.62%\n",
      "Run: 01, Epoch: 200, Loss: 0.8107, Train: 75.79%, Valid: 72.27% Test: 71.60%\n",
      "Run: 01, Epoch: 210, Loss: 0.8044, Train: 76.19%, Valid: 71.50% Test: 70.22%\n",
      "Run: 01, Epoch: 220, Loss: 0.7990, Train: 76.58%, Valid: 71.26% Test: 69.20%\n",
      "Run: 01, Epoch: 230, Loss: 0.7921, Train: 76.90%, Valid: 72.57% Test: 71.79%\n",
      "Run: 01, Epoch: 240, Loss: 0.7859, Train: 77.25%, Valid: 71.74% Test: 70.14%\n",
      "Run: 01, Epoch: 250, Loss: 0.7793, Train: 76.73%, Valid: 71.93% Test: 70.99%\n",
      "Run: 01, Epoch: 260, Loss: 0.7762, Train: 77.64%, Valid: 72.60% Test: 71.12%\n",
      "Run: 01, Epoch: 270, Loss: 0.7721, Train: 77.61%, Valid: 72.79% Test: 71.16%\n",
      "Run: 01, Epoch: 280, Loss: 0.7679, Train: 77.58%, Valid: 72.85% Test: 72.33%\n",
      "Run: 01, Epoch: 290, Loss: 0.7623, Train: 77.73%, Valid: 71.88% Test: 69.69%\n",
      "Run: 01, Epoch: 300, Loss: 0.7552, Train: 77.82%, Valid: 72.92% Test: 71.83%\n",
      "Run: 01, Epoch: 310, Loss: 0.7511, Train: 78.03%, Valid: 72.41% Test: 71.20%\n",
      "Run: 01, Epoch: 320, Loss: 0.7453, Train: 78.00%, Valid: 71.11% Test: 68.59%\n",
      "Run: 01, Epoch: 330, Loss: 0.7435, Train: 77.91%, Valid: 72.43% Test: 71.15%\n",
      "Run: 01, Epoch: 340, Loss: 0.7424, Train: 78.46%, Valid: 72.20% Test: 70.57%\n",
      "Run: 01, Epoch: 350, Loss: 0.7374, Train: 78.53%, Valid: 72.85% Test: 72.13%\n",
      "Run: 01, Epoch: 360, Loss: 0.7354, Train: 78.31%, Valid: 71.60% Test: 69.38%\n",
      "Run: 01, Epoch: 370, Loss: 0.7299, Train: 78.59%, Valid: 71.86% Test: 69.96%\n",
      "Run: 01, Epoch: 380, Loss: 0.7263, Train: 78.90%, Valid: 73.01% Test: 71.51%\n",
      "Run: 01, Epoch: 390, Loss: 0.7255, Train: 78.82%, Valid: 72.84% Test: 71.98%\n",
      "Run: 01, Epoch: 400, Loss: 0.7234, Train: 78.55%, Valid: 72.63% Test: 72.11%\n",
      "Run: 01, Epoch: 410, Loss: 0.7184, Train: 79.23%, Valid: 72.29% Test: 70.49%\n",
      "Run: 01, Epoch: 420, Loss: 0.7184, Train: 78.72%, Valid: 71.81% Test: 69.32%\n",
      "Run: 01, Epoch: 430, Loss: 0.7105, Train: 79.35%, Valid: 72.14% Test: 70.07%\n",
      "Run: 01, Epoch: 440, Loss: 0.7105, Train: 79.32%, Valid: 72.29% Test: 69.97%\n",
      "Run: 01, Epoch: 450, Loss: 0.7072, Train: 79.69%, Valid: 72.85% Test: 70.95%\n",
      "Run: 01, Epoch: 460, Loss: 0.7029, Train: 79.07%, Valid: 71.23% Test: 68.65%\n",
      "Run: 01, Epoch: 470, Loss: 0.7036, Train: 79.34%, Valid: 71.35% Test: 69.01%\n",
      "Run: 01, Epoch: 480, Loss: 0.7025, Train: 79.21%, Valid: 71.09% Test: 68.41%\n",
      "Run: 01, Epoch: 490, Loss: 0.6997, Train: 80.13%, Valid: 72.72% Test: 70.70%\n",
      "Run: 01, Epoch: 500, Loss: 0.6981, Train: 79.29%, Valid: 72.14% Test: 71.36%\n",
      "Best test accuracy: 72.33%\n"
     ]
    }
   ],
   "source": [
    "data.adj_t = torch_sparse.fill_diag(data.adj_t, 1)\n",
    "deg = torch_sparse.sum(data.adj_t, 0).pow_(-0.5)\n",
    "data.adj_t = torch_sparse.mul(data.adj_t, deg.view(-1, 1))\n",
    "data.adj_t = torch_sparse.mul(data.adj_t, deg.view(1, -1))\n",
    "\n",
    "test_scores = []\n",
    "for epoch in range(1, 1 + epochs):\n",
    "    loss = train(model, data, train_idx, optimizer, criterion)\n",
    "    result = test(model, data, split_idx, evaluator)\n",
    "\n",
    "    if epoch % log_steps == 0:\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        test_scores.append(test_acc)\n",
    "        print(f'Run: {1:02d}, '\n",
    "              f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_acc:.2f}%, '\n",
    "              f'Valid: {100 * valid_acc:.2f}% '\n",
    "              f'Test: {100 * test_acc:.2f}%')\n",
    "print(f\"Best test accuracy: {max(test_scores) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110120\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
